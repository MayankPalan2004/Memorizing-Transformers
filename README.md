# Memorizing-Transformers
Pytorch Implementation of Memorizing Transformers by Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy [https://arxiv.org/abs/2203.08913]
The "Memorizing Transformers" paper introduces the Memory-Augmented Transformer (MAT) architecture, integrating a Memory Attention Module (MAM) utilizing k-nearest neighbors (KNN) search for efficient retrieval of relevant information from memory. This enhances the transformer's ability to handle tasks requiring memorization by facilitating effective storage and retrieval of key information during decoding.

![2744B704-37A6-4619-9F4B-9E12B6F0AA96](https://github.com/MayankPalan2004/Memorizing-Transformers/assets/144169682/c0bbbf08-86b9-431b-88b6-a9db2629be69)

# Results 
Trained for 160 epochs where the loss dropped from 5.03 to 2.39

![744F0981-FA6C-488E-8D56-5E58D1AEDFF5_4_5005_c](https://github.com/MayankPalan2004/Memorizing-Transformers/assets/144169682/b7acab44-3d9d-4f53-bcf3-120737ac0b31)

![286CF399-8674-4791-B3EF-5E95B74B60D4_4_5005_c](https://github.com/MayankPalan2004/Memorizing-Transformers/assets/144169682/9046913a-d8cf-44f2-adf4-e8259d3554c4)











