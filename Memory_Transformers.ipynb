{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Memory Transformers",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYUDOWKn8rXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DGOBcuc08rXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import time\n",
        "\n",
        "!pip install einops\n",
        "from einops import rearrange, repeat, pack, unpack, einsum\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "\n",
        "from functools import partial, wraps\n",
        "from contextlib import contextmanager, ExitStack\n",
        "from pathlib import Path\n",
        "from filelock import FileLock\n",
        "import pickle\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "!pip install faiss-gpu\n",
        "import faiss\n",
        "\n",
        "!pip install datasets\n",
        "import datasets"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-17T13:46:06.597299Z",
          "iopub.execute_input": "2024-03-17T13:46:06.597936Z",
          "iopub.status.idle": "2024-03-17T13:46:58.379449Z",
          "shell.execute_reply.started": "2024-03-17T13:46:06.597904Z",
          "shell.execute_reply": "2024-03-17T13:46:58.37848Z"
        },
        "trusted": true,
        "id": "Kfu92QPs8rX1",
        "outputId": "0bd86b1d-3c3f-4f32-9145-fc7caf94b68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This line of code checks whether a CUDA-enabled GPU is available on the system.\n",
        "If a GPU is available, it sets the device to 'cuda'; otherwise, it sets it to 'cpu'.\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T13:47:39.37598Z",
          "iopub.execute_input": "2024-03-17T13:47:39.376396Z",
          "iopub.status.idle": "2024-03-17T13:47:39.382659Z",
          "shell.execute_reply.started": "2024-03-17T13:47:39.376365Z",
          "shell.execute_reply": "2024-03-17T13:47:39.381507Z"
        },
        "trusted": true,
        "id": "4BI_BrDs8rX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RelativePosition(nn.Module):\n",
        "    \"\"\"\n",
        "    RelativePosition module computes relative positions between sequence elements and generates corresponding relative position embeddings.\n",
        "\n",
        "    Args:\n",
        "        rp_scale (float): Scaling factor for the relative position embeddings.\n",
        "        num_buckets (int): Number of buckets for discretizing relative positions.\n",
        "        rp_max_distance (int): Maximum relative distance considered.\n",
        "        heads (int): Number of attention heads.\n",
        "\n",
        "    Attributes:\n",
        "        scale (float): Scaling factor for the relative position embeddings.\n",
        "        num_buckets (int): Number of buckets for discretizing relative positions.\n",
        "        rp_max_distance (int): Maximum relative distance considered.\n",
        "        relative_attention_embedding (torch.nn.Embedding): Embedding layer for storing relative position embeddings.\n",
        "\n",
        "    Methods:\n",
        "        relative_position_bucket: Computes bucket indices for relative positions.\n",
        "        forward: Forward pass of the module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rp_scale,\n",
        "        num_buckets=32,\n",
        "        rp_max_distance=128,\n",
        "        heads=8\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the RelativePosition module.\n",
        "\n",
        "        Args:\n",
        "            rp_scale (float): Scaling factor for the relative position embeddings.\n",
        "            num_buckets (int): Number of buckets for discretizing relative positions.\n",
        "            rp_max_distance (int): Maximum relative distance considered.\n",
        "            heads (int): Number of attention heads.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.scale = rp_scale\n",
        "        self.num_buckets = num_buckets\n",
        "        self.rp_max_distance = rp_max_distance\n",
        "        self.relative_attention_embedding = nn.Embedding(num_buckets, heads)\n",
        "\n",
        "    def relative_position_bucket(self, relative_position_matrix):\n",
        "        \"\"\"\n",
        "        Computes bucket indices for given relative position matrix.\n",
        "\n",
        "        Args:\n",
        "            relative_position_matrix (torch.Tensor): Matrix of relative positions.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Bucket indices for the given relative positions.\n",
        "        \"\"\"\n",
        "        n = -relative_position_matrix\n",
        "        n = torch.max(n, torch.zeros_like(n))\n",
        "\n",
        "        max_exact = self.num_buckets // 2\n",
        "\n",
        "        is_small = n < max_exact\n",
        "        val_if_large = max_exact + (torch.log(n.float() / max_exact) / math.log(self.rp_max_distance / max_exact) * (self.num_buckets - max_exact)).long()\n",
        "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, self.num_buckets - 1))\n",
        "\n",
        "        return torch.where(is_small, n, val_if_large)\n",
        "\n",
        "    def forward(self, sequence_length, device):\n",
        "        \"\"\"\n",
        "        Forward pass of the RelativePosition module.\n",
        "\n",
        "        Args:\n",
        "            sequence_length (int): Length of the sequence.\n",
        "            device (torch.device): Device to perform computations.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Relative position embeddings scaled by the scaling factor.\n",
        "        \"\"\"\n",
        "        sequence_pos = torch.arange(sequence_length, dtype=torch.long, device=device)\n",
        "        context_pos = torch.arange(2 * sequence_length, dtype=torch.long, device=device)\n",
        "        sequence_rel_pos = rearrange(sequence_pos, 'i -> i 1')\n",
        "        context_rel_pos = rearrange(context_pos, 'j -> 1 j')\n",
        "        rel_pos = context_rel_pos - sequence_rel_pos\n",
        "\n",
        "        position_bucket_indices = self.relative_position_bucket(rel_pos)\n",
        "\n",
        "        rp_values = self.relative_attention_embedding(position_bucket_indices)\n",
        "        rp_values = rearrange(rp_values, 'i j h -> () h i j')\n",
        "        return rp_values * self.scale"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T13:49:06.902381Z",
          "iopub.execute_input": "2024-03-17T13:49:06.902775Z",
          "iopub.status.idle": "2024-03-17T13:49:06.920875Z",
          "shell.execute_reply.started": "2024-03-17T13:49:06.902745Z",
          "shell.execute_reply": "2024-03-17T13:49:06.919605Z"
        },
        "trusted": true,
        "id": "7nce4LzI8rX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN():\n",
        "    \"\"\"\n",
        "    KNN (K-Nearest Neighbors) class for efficient nearest neighbor search.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimensionality of the data vectors.\n",
        "        max_memories (int): Maximum number of data vectors to be stored in memory.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): Dimensionality of the data vectors.\n",
        "        max_memories (int): Maximum number of data vectors to be stored in memory.\n",
        "        shape (tuple): Shape of the memory map storing the data vectors.\n",
        "        db_offset (int): Offset for indexing the memory map.\n",
        "        db_filepath (str): Filepath for the memory map.\n",
        "        db (np.memmap): Memory map for storing data vectors.\n",
        "        index (faiss.IndexFlatL2): FAISS index for fast nearest neighbor search.\n",
        "\n",
        "    Methods:\n",
        "        add_to_db: Add new data vectors to the memory map.\n",
        "        search_and_retrieve: Perform nearest neighbor search on query vectors.\n",
        "        add: Add new data vectors to the memory map and update the index.\n",
        "        search: Perform nearest neighbor search on query vectors.\n",
        "        clear: Clear the memory map and reset the index.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        max_memories,\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the KNN object.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Dimensionality of the data vectors.\n",
        "            max_memories (int): Maximum number of data vectors to be stored in memory.\n",
        "        \"\"\"\n",
        "        self.dim = dim\n",
        "        self.max_memories = max_memories\n",
        "        self.shape = (max_memories, 2, dim)\n",
        "        self.db_offset = 0\n",
        "        self.db_filepath = \"./memory.memmap\"\n",
        "        self.db = np.memmap(self.db_filepath, mode='w+', dtype=np.float32, shape=self.shape)\n",
        "        self.index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "    def add_to_db(self, new_data):\n",
        "        \"\"\"\n",
        "        Add new data vectors to the memory map.\n",
        "\n",
        "        Args:\n",
        "            new_data (torch.Tensor): New data vectors to be added.\n",
        "        \"\"\"\n",
        "        new_data_len = new_data.shape[0]\n",
        "        ids = (np.arange(new_data_len) + self.db_offset)\n",
        "        self.db[ids] = new_data.detach().cpu().numpy()\n",
        "        self.db_offset += new_data_len\n",
        "        # Write to file\n",
        "        self.db.flush()\n",
        "\n",
        "    def search_and_retrieve(self, query_vecs, topk):\n",
        "        \"\"\"\n",
        "        Perform nearest neighbor search on query vectors.\n",
        "\n",
        "        Args:\n",
        "            query_vecs (np.ndarray): Query vectors for nearest neighbor search.\n",
        "            topk (int): Number of nearest neighbors to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Nearest neighbor key-value pairs.\n",
        "        \"\"\"\n",
        "        query_vecs = query_vecs\n",
        "        distances, indices = self.index.search(query_vecs, topk)\n",
        "        kvs = self.db[indices]\n",
        "        return kvs\n",
        "\n",
        "    def add(self, new_data):\n",
        "        \"\"\"\n",
        "        Add new data vectors to the memory map and update the index.\n",
        "\n",
        "        Args:\n",
        "            new_data (torch.Tensor): New data vectors to be added.\n",
        "        \"\"\"\n",
        "        # Input is b n 2 d, flatten to (b n) 2 d\n",
        "        new_data = new_data.flatten(0, 1)\n",
        "        # Add to db\n",
        "        self.add_to_db(new_data)\n",
        "        # Only keys are used in knn index\n",
        "        keys, vals = new_data.unbind(dim=-2)\n",
        "        keys = keys.detach().cpu().numpy()\n",
        "        # Add (b n) d tensors to index\n",
        "        keys = np.ascontiguousarray(keys)\n",
        "        # Add to index\n",
        "        self.index.add(keys)\n",
        "\n",
        "    def search(self, query_vecs, topk):\n",
        "        \"\"\"\n",
        "        Perform nearest neighbor search on query vectors.\n",
        "\n",
        "        Args:\n",
        "            query_vecs (torch.Tensor): Query vectors for nearest neighbor search.\n",
        "            topk (int): Number of nearest neighbors to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Nearest neighbor key-value pairs.\n",
        "        \"\"\"\n",
        "        query_batch_size, query_seq_len = query_vecs.shape[0], query_vecs.shape[1]\n",
        "        device = query_vecs.device\n",
        "        # Input is b n d, flatten to (b n) d\n",
        "        query_vecs = query_vecs.flatten(0, 1)\n",
        "        kvs = self.search_and_retrieve(np.ascontiguousarray(query_vecs.detach().cpu().numpy()), topk)\n",
        "        # kvs are (b n) k 2 d, unflatten to b n k 2 d\n",
        "        kvs = torch.tensor(kvs)\n",
        "        kvs = torch.unflatten(kvs, 0, (query_batch_size, query_seq_len))\n",
        "        return kvs.to(device)\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"\n",
        "        Clear the memory map and reset the index.\n",
        "        \"\"\"\n",
        "        self.index.reset()\n",
        "        self.db[:] = 0\n",
        "        self.db_offset = 0\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T13:50:57.618616Z",
          "iopub.execute_input": "2024-03-17T13:50:57.619705Z",
          "iopub.status.idle": "2024-03-17T13:50:57.635172Z",
          "shell.execute_reply.started": "2024-03-17T13:50:57.619667Z",
          "shell.execute_reply": "2024-03-17T13:50:57.634192Z"
        },
        "trusted": true,
        "id": "ZEwj2WAZ8rX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class XLAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    XLAttention module implements the attention mechanism used in XLNet.\n",
        "\n",
        "    Args:\n",
        "        embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "        heads (int): Number of attention heads.\n",
        "        head_dimension (int): Dimensionality of each attention head.\n",
        "        dropout (float): Dropout probability.\n",
        "\n",
        "    Attributes:\n",
        "        heads (int): Number of attention heads.\n",
        "        dropout (nn.Dropout): Dropout layer.\n",
        "        scale (float): Scaling factor for attention scores.\n",
        "        query_matrix (nn.Linear): Linear layer for query projection.\n",
        "        key_matrix (nn.Linear): Linear layer for key projection.\n",
        "        value_matrix (nn.Linear): Linear layer for value projection.\n",
        "        output_matrix (nn.Linear): Linear layer for output projection.\n",
        "\n",
        "    Methods:\n",
        "        forward: Forward pass of the XLAttention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        heads=8,\n",
        "        head_dimension=64,\n",
        "        dropout=0.,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the XLAttention module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "            heads (int): Number of attention heads.\n",
        "            head_dimension (int): Dimensionality of each attention head.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = head_dimension ** -0.5\n",
        "\n",
        "        self.query_matrix = nn.Linear(embedding_dimension, self.heads * head_dimension)\n",
        "        self.key_matrix = nn.Linear(embedding_dimension, self.heads * head_dimension)\n",
        "        self.value_matrix = nn.Linear(embedding_dimension, self.heads * head_dimension)\n",
        "        self.output_matrix = nn.Linear(self.heads * head_dimension, embedding_dimension)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,  # batch_size, sequence_length, embedding_dimension\n",
        "        relative_positions=None,\n",
        "        xl_memory=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass of the XLAttention module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dimension).\n",
        "            relative_positions (torch.Tensor): Tensor containing relative positions.\n",
        "            xl_memory (torch.Tensor): XL memory for cross-attention.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "            torch.Tensor: XL memory to add.\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        queries = self.query_matrix(x)\n",
        "        keys = self.key_matrix(x)\n",
        "        values = self.value_matrix(x)\n",
        "\n",
        "        queries = queries * self.scale\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            k_xl, v_xl = xl_memory.unbind(dim=-2)\n",
        "            keys = torch.cat((k_xl, keys), dim=-2)\n",
        "            values = torch.cat((v_xl, values), dim=-2)\n",
        "            xl_sequence_length = k_xl.shape[1]\n",
        "\n",
        "        queries = rearrange(queries, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        keys = rearrange(keys, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qk = einsum(queries, keys, 'b h i d, b h j d -> b h i j')\n",
        "\n",
        "        i, j = qk.shape[-2:]\n",
        "        if relative_positions is not None:\n",
        "            qk = relative_positions[..., -i:, -j:] + qk\n",
        "\n",
        "        qk = qk * self.scale\n",
        "\n",
        "        mask = torch.ones((i, j), dtype=torch.bool, device=device).triu(j - i + 1)\n",
        "        qk = qk.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        qk = F.softmax(qk, dim=-1)\n",
        "        qk = self.dropout(qk)\n",
        "\n",
        "        values = rearrange(values, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qkv = qk @ values\n",
        "        qkv = rearrange(qkv, 'b h t d -> b t (h d)')\n",
        "\n",
        "        out = self.output_matrix(qkv)\n",
        "\n",
        "        keys = rearrange(keys, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        values = rearrange(values, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        kv_memories = torch.stack((keys, values), dim=-2)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            xl_memories, current_input = kv_memories[:, :-xl_sequence_length], kv_memories[:, -xl_sequence_length:]\n",
        "            kv_to_add_xl = current_input\n",
        "        else:\n",
        "            kv_to_add_xl = kv_memories\n",
        "\n",
        "        return out, kv_to_add_xl\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:17:32.555844Z",
          "iopub.execute_input": "2024-03-17T14:17:32.556493Z",
          "iopub.status.idle": "2024-03-17T14:17:32.573871Z",
          "shell.execute_reply.started": "2024-03-17T14:17:32.55646Z",
          "shell.execute_reply": "2024-03-17T14:17:32.572833Z"
        },
        "trusted": true,
        "id": "Fc3VdQli8rX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    KNNAttention module implements the attention mechanism with K-Nearest Neighbors (KNN) retrieval.\n",
        "\n",
        "    Args:\n",
        "        embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "        knn (KNN): KNN object for memory retrieval.\n",
        "        heads (int): Number of attention heads.\n",
        "        head_dimension (int): Dimensionality of each attention head.\n",
        "        topk_retrieved_memories (int): Number of top memories to retrieve.\n",
        "        dropout (float): Dropout probability.\n",
        "\n",
        "    Attributes:\n",
        "        heads (int): Number of attention heads.\n",
        "        scale (float): Scaling factor for attention scores.\n",
        "        dropout (nn.Dropout): Dropout layer.\n",
        "        query_matrix (nn.Linear): Linear layer for query projection.\n",
        "        key_matrix (nn.Linear): Linear layer for key projection.\n",
        "        value_matrix (nn.Linear): Linear layer for value projection.\n",
        "        output_matrix (nn.Linear): Linear layer for output projection.\n",
        "        gate_bias (nn.Parameter): Learnable bias for gating mechanism.\n",
        "        topk_retrieved_memories (int): Number of top memories to retrieve.\n",
        "        knn (KNN): KNN object for memory retrieval.\n",
        "\n",
        "    Methods:\n",
        "        forward: Forward pass of the KNNAttention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        knn,\n",
        "        heads=8,\n",
        "        head_dimension=64,\n",
        "        topk_retrieved_memories=3,\n",
        "        dropout=0.\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the KNNAttention module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "            knn (KNN): KNN object for memory retrieval.\n",
        "            heads (int): Number of attention heads.\n",
        "            head_dimension (int): Dimensionality of each attention head.\n",
        "            topk_retrieved_memories (int): Number of top memories to retrieve.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = head_dimension ** -0.5\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.query_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.key_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.value_matrix = nn.Linear(embedding_dimension, heads * head_dimension)\n",
        "        self.output_matrix = nn.Linear(heads * head_dimension, embedding_dimension)\n",
        "\n",
        "        self.gate_bias = nn.Parameter(torch.randn(self.heads, 1, 1))\n",
        "        self.topk_retrieved_memories = topk_retrieved_memories\n",
        "        self.knn = knn\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,  # batch_size, sequence_length, embedding_dimension\n",
        "        relative_positions=None,\n",
        "        xl_memory=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass of the KNNAttention module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dimension).\n",
        "            relative_positions (torch.Tensor): Tensor containing relative positions.\n",
        "            xl_memory (torch.Tensor): XL memory for cross-attention.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "            torch.Tensor: XL memory to add.\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        batch_size, sequence_length = x.shape[:2]\n",
        "        queries = self.query_matrix(x)\n",
        "        keys = self.key_matrix(x)\n",
        "        values = self.value_matrix(x)\n",
        "\n",
        "        queries = F.normalize(queries, dim=-1)\n",
        "        keys = F.normalize(keys, dim=-1)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            k_xl, v_xl = xl_memory.unbind(dim=-2)\n",
        "            keys = torch.cat((k_xl, keys), dim=-2)\n",
        "            values = torch.cat((v_xl, values), dim=-2)\n",
        "            xl_sequence_length = k_xl.shape[1]\n",
        "\n",
        "        queries = rearrange(queries, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        keys = rearrange(keys, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qk = einsum(queries, keys, 'b h i d, b h j d -> b h i j')\n",
        "\n",
        "        i, j = qk.shape[-2:]\n",
        "        if relative_positions is not None:\n",
        "            qk = relative_positions[..., -i:, -j:] + qk\n",
        "\n",
        "        qk = qk * self.scale\n",
        "\n",
        "        mask = torch.ones((i, j), dtype=torch.bool, device=device).triu(j - i + 1)\n",
        "        qk = qk.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        qk = F.softmax(qk, dim=-1)\n",
        "\n",
        "        qk = self.dropout(qk)\n",
        "\n",
        "        values = rearrange(values, 'b t (h d) -> b h t d', h=self.heads)\n",
        "        qkv = qk @ values\n",
        "\n",
        "        if self.knn.index.ntotal > 0:\n",
        "            queries = rearrange(queries, 'b h t d -> b t (h d)')\n",
        "            mem_kv = self.knn.search(queries, topk=self.topk_retrieved_memories)\n",
        "            mem_k, mem_v = mem_kv.unbind(dim=-2)\n",
        "            mem_k = rearrange(mem_k, 'b t k (h d) -> b h t k d', h=self.heads)\n",
        "            mem_v = rearrange(mem_v, 'b t k (h d) -> b h t k d', h=self.heads)\n",
        "\n",
        "            queries = rearrange(queries, 'b t (h d) -> b h t d', h=self.heads)\n",
        "            mem_qk = einsum(queries, mem_k, 'b h t d, b h t k d -> b h t k')\n",
        "            mem_qk = mem_qk * self.scale\n",
        "\n",
        "            mem_qk = F.softmax(mem_qk, dim=-1)\n",
        "            mem_qk = self.dropout(mem_qk)\n",
        "            mem_qkv = einsum(mem_qk, mem_v, 'b h t k, b h t k d -> b h t d')\n",
        "\n",
        "            combined_qkv = mem_qkv * self.gate_bias + qkv * (1 - self.gate_bias)\n",
        "            combined_qkv = rearrange(combined_qkv, 'b h t d -> b t (h d)')\n",
        "            out = self.output_matrix(combined_qkv)\n",
        "        else:\n",
        "            qkv = rearrange(qkv, 'b h t d -> b t (h d)')\n",
        "            out = self.output_matrix(qkv)\n",
        "\n",
        "        keys = rearrange(keys, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        values = rearrange(values, 'b h t d -> b t (h d)', h=self.heads)\n",
        "        kv_memories = torch.stack((keys, values), dim=-2)\n",
        "\n",
        "        if xl_memory is not None:\n",
        "            xl_memories, current_kv = kv_memories[:, :-xl_sequence_length], kv_memories[:, -xl_sequence_length:]\n",
        "        else:\n",
        "            current_kv = kv_memories\n",
        "\n",
        "        self.knn.add(current_kv)\n",
        "\n",
        "        return out, current_kv"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:17:32.760461Z",
          "iopub.execute_input": "2024-03-17T14:17:32.760812Z",
          "iopub.status.idle": "2024-03-17T14:17:32.784093Z",
          "shell.execute_reply.started": "2024-03-17T14:17:32.760788Z",
          "shell.execute_reply": "2024-03-17T14:17:32.783Z"
        },
        "trusted": true,
        "id": "tqbvS4OR8rX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Block module defines a single block in the transformer architecture.\n",
        "\n",
        "    Args:\n",
        "        embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "        attention_type (nn.Module): Type of attention mechanism to be used.\n",
        "        dropout (float): Dropout probability.\n",
        "\n",
        "    Attributes:\n",
        "        attention (nn.Module): Attention mechanism.\n",
        "        dim (int): Dimensionality of the input embeddings.\n",
        "        norm (nn.LayerNorm): Layer normalization.\n",
        "        ff_block (nn.Sequential): Feed-forward block.\n",
        "\n",
        "    Methods:\n",
        "        forward: Forward pass of the Block module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dimension, attention_type, dropout=0.):\n",
        "        \"\"\"\n",
        "        Initializes the Block module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "            attention_type (nn.Module): Type of attention mechanism to be used.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.attention = attention_type\n",
        "        self.dim = embedding_dimension\n",
        "        self.norm = nn.LayerNorm(self.dim)\n",
        "\n",
        "        self.ff_block = nn.Sequential(\n",
        "            nn.LayerNorm(self.dim),\n",
        "            nn.Linear(self.dim, self.dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.dim * 4, self.dim))\n",
        "\n",
        "    def forward(self, x, xl_memories, rel_pos):\n",
        "        \"\"\"\n",
        "        Forward pass of the Block module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            xl_memories (torch.Tensor): XL memory for cross-attention.\n",
        "            rel_pos (torch.Tensor): Tensor containing relative positions.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "            torch.Tensor: New XL memory.\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        attn_out = self.norm(x)\n",
        "        attn_out, new_xl_memories = self.attention(attn_out, relative_positions=rel_pos, xl_memory=xl_memories)\n",
        "        attn_out += residual\n",
        "\n",
        "        residual = attn_out\n",
        "        ff_out = self.ff_block(attn_out)\n",
        "        ff_out += residual\n",
        "        return ff_out, new_xl_memories"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:17:32.961241Z",
          "iopub.execute_input": "2024-03-17T14:17:32.961598Z",
          "iopub.status.idle": "2024-03-17T14:17:32.971589Z",
          "shell.execute_reply.started": "2024-03-17T14:17:32.961572Z",
          "shell.execute_reply": "2024-03-17T14:17:32.970617Z"
        },
        "trusted": true,
        "id": "elPGkKu08rX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemorizingTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    MemorizingTransformer module implements a transformer with memory capabilities.\n",
        "\n",
        "    Args:\n",
        "        embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        max_knn_memories (int): Maximum number of KNN memories.\n",
        "        heads (int): Number of attention heads.\n",
        "        depth (int): Depth of the transformer.\n",
        "        dropout (float): Dropout probability.\n",
        "        head_dimension (int): Dimensionality of each attention head.\n",
        "        topk (int): Number of top memories to retrieve.\n",
        "\n",
        "    Attributes:\n",
        "        heads (int): Number of attention heads.\n",
        "        embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "        dropout (float): Dropout probability.\n",
        "        depth (int): Depth of the transformer.\n",
        "        head_dimension (int): Dimensionality of each attention head.\n",
        "        max_knn_memories (int): Maximum number of KNN memories.\n",
        "        topk (int): Number of top memories to retrieve.\n",
        "        rel_pos (RelativePosition): Relative position embedding for XLAttention.\n",
        "        rel_pos_knn (RelativePosition): Relative position embedding for KNNAttention.\n",
        "        embedding_matrix (nn.Embedding): Embedding matrix.\n",
        "        knn (KNN): KNN object for memory retrieval.\n",
        "        layers (nn.ModuleList): List of transformer blocks.\n",
        "        to_logits (nn.Sequential): Final linear layer for logits.\n",
        "\n",
        "    Methods:\n",
        "        forward: Forward pass of the MemorizingTransformer module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        vocab_size,\n",
        "        max_knn_memories=81920,\n",
        "        heads=8,\n",
        "        depth=10,\n",
        "        dropout=0,\n",
        "        head_dimension=64,\n",
        "        topk=5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the MemorizingTransformer module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dimension (int): Dimensionality of the input embeddings.\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            max_knn_memories (int): Maximum number of KNN memories.\n",
        "            heads (int): Number of attention heads.\n",
        "            depth (int): Depth of the transformer.\n",
        "            dropout (float): Dropout probability.\n",
        "            head_dimension (int): Dimensionality of each attention head.\n",
        "            topk (int): Number of top memories to retrieve.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.dropout = dropout\n",
        "        self.depth = depth\n",
        "        self.head_dimension = head_dimension\n",
        "        self.max_knn_memories = max_knn_memories\n",
        "        self.topk = topk\n",
        "\n",
        "        self.rel_pos = RelativePosition(rp_scale=head_dimension ** 0.5, heads=self.heads)\n",
        "        self.rel_pos_knn = RelativePosition(rp_scale=head_dimension ** 0.5, heads=self.heads)\n",
        "        self.embedding_matrix = nn.Embedding(vocab_size, self.embedding_dimension)\n",
        "        self.knn = KNN(head_dimension * heads, self.max_knn_memories)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for i in range(self.depth):\n",
        "            if i == self.depth - 2:\n",
        "                attention_type = KNNAttention(\n",
        "                    self.embedding_dimension,\n",
        "                    self.knn,\n",
        "                    heads=self.heads,\n",
        "                    head_dimension=self.head_dimension,\n",
        "                    dropout=self.dropout\n",
        "                )\n",
        "            else:\n",
        "                attention_type = XLAttention(\n",
        "                    self.embedding_dimension,\n",
        "                    heads=self.heads,\n",
        "                    head_dimension=self.head_dimension,\n",
        "                    dropout=self.dropout\n",
        "                )\n",
        "            self.layers.append(Block(self.embedding_dimension, attention_type))\n",
        "\n",
        "        self.to_logits = nn.Sequential(\n",
        "            nn.LayerNorm(self.embedding_dimension),\n",
        "            nn.Linear(self.embedding_dimension, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        relative_positions=None,\n",
        "        xl_memories=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass of the MemorizingTransformer module.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            relative_positions (torch.Tensor): Tensor containing relative positions.\n",
        "            xl_memories (torch.Tensor): XL memory for cross-attention.\n",
        "            labels (torch.Tensor): Target labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss tensor.\n",
        "            list: List of new XL memories.\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        batch_size, sequence_length = x.shape[0], x.shape[1]\n",
        "\n",
        "        rel_pos = self.rel_pos(sequence_length, device=device)\n",
        "        rel_pos_knn = self.rel_pos_knn(sequence_length, device=device)\n",
        "\n",
        "        if xl_memories is not None:\n",
        "            xl_memories = xl_memories\n",
        "        else:\n",
        "            xl_memories = (None,) * self.depth\n",
        "\n",
        "        xl_memories_iter = iter(xl_memories)\n",
        "\n",
        "        x = self.embedding_matrix(x)\n",
        "        new_xl_memories = []\n",
        "\n",
        "        for ind, block in enumerate(self.layers):\n",
        "            if ind == self.depth - 2:\n",
        "                layer_rel_pos = rel_pos_knn\n",
        "            else:\n",
        "                layer_rel_pos = rel_pos\n",
        "\n",
        "            x, xl_mem = block(x, next(xl_memories_iter), layer_rel_pos)\n",
        "\n",
        "            if xl_mem is not None:\n",
        "                new_xl_memories.append(xl_mem.detach())\n",
        "\n",
        "        logits = self.to_logits(x)\n",
        "        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), labels)\n",
        "\n",
        "        if len(new_xl_memories) > 0:\n",
        "            return loss, new_xl_memories\n",
        "        return loss\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:17:33.384715Z",
          "iopub.execute_input": "2024-03-17T14:17:33.385097Z",
          "iopub.status.idle": "2024-03-17T14:17:33.404293Z",
          "shell.execute_reply.started": "2024-03-17T14:17:33.385067Z",
          "shell.execute_reply": "2024-03-17T14:17:33.403401Z"
        },
        "trusted": true,
        "id": "Qo3t3G4a8rX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "T0P1axiW8rX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "SEGMENTS = 10\n",
        "SEQUENCE_LENGTH = 512\n",
        "CHUNK_SIZE = (SEGMENTS * SEQUENCE_LENGTH) + 1\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"ccdv/arxiv-summarization\", split='train', streaming=True)\n",
        "raw_dataset = list(dataset.take(3500))\n",
        "\n",
        "# Extract raw articles\n",
        "raw_articles = [x['article'] for x in raw_dataset]\n",
        "raw_articles = [x for x in raw_articles if len(x) > CHUNK_SIZE]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "converted = [np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]\n",
        "\n",
        "# Clip articles to CHUNK_SIZE\n",
        "def clip_article(doc, chunk_size):\n",
        "    remainder = len(doc) % chunk_size\n",
        "    return doc[:-remainder]\n",
        "\n",
        "clipped = [clip_article(doc, CHUNK_SIZE) for doc in converted]\n",
        "\n",
        "\n",
        "\n",
        "# Ensure all documents have the same shape\n",
        "min_length = min(len(doc) for doc in clipped)\n",
        "clipped = [doc[:min_length] for doc in clipped]\n",
        "\n",
        "# Reshape the documents\n",
        "chunked = np.array([doc.reshape(-1, CHUNK_SIZE) for doc in clipped])\n",
        "\n",
        "# Convert to torch tensor\n",
        "processed_data = torch.tensor(np.concatenate(chunked), dtype=torch.long)\n",
        "\n",
        "# Split into train, validation, and test loaders\n",
        "eighty_split = int(processed_data.shape[0] * .8)\n",
        "ninety_split = int(processed_data.shape[0] * .9)\n",
        "\n",
        "train_loader = DataLoader(processed_data[:eighty_split], batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(processed_data[eighty_split:ninety_split], batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(processed_data[ninety_split:], batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Check the shape of processed_data\n",
        "print(\"Processed Data Shape:\", processed_data.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:16:34.199026Z",
          "iopub.execute_input": "2024-03-17T14:16:34.199472Z",
          "iopub.status.idle": "2024-03-17T14:16:38.90971Z",
          "shell.execute_reply.started": "2024-03-17T14:16:34.199442Z",
          "shell.execute_reply": "2024-03-17T14:16:38.908742Z"
        },
        "trusted": true,
        "id": "WJtv3CJK8rX-",
        "outputId": "e2e23b8d-7772-482a-ab15-9af785ad3fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Processed Data Shape: torch.Size([3401, 5121])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_34/3020860341.py:20: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n  converted = [np.fromstring(doc, dtype=np.uint8) for doc in raw_articles]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MemorizingTransformer(embedding_dimension = 128,\n",
        "                              vocab_size = 128,\n",
        "                              max_knn_memories = MAX_KNN_MEMORIES)\n",
        "\n",
        "model.to(device) ###########\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "model.train()\n",
        "\n",
        "for i in tqdm.tqdm(range(200), mininterval = 10., desc = 'training'):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "    # Clear XL memories\n",
        "    xl_memories = None\n",
        "    # Clear KNN memory\n",
        "    model.knn.clear()\n",
        "\n",
        "    data = next(iter(train_loader)).to(device=device)\n",
        "    seq, labels = data[:, :-1], data[:, 1:]\n",
        "\n",
        "    t0 = time.time()\n",
        "    print (\"Begin document\")\n",
        "\n",
        "    # Each pass will be (BATCH_SIZE * SEGMENTS) iterations\n",
        "    for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
        "\n",
        "        loss, xl_memories = model(\n",
        "            seq_segment,\n",
        "            labels = labels_segment,\n",
        "            xl_memories = xl_memories\n",
        "        )\n",
        "\n",
        "        train_loss += loss.item() / SEGMENTS\n",
        "        (loss / SEGMENTS).backward()\n",
        "\n",
        "\n",
        "    print(f'training loss: {train_loss}')\n",
        "    t1 = time.time()\n",
        "    print (\"End document, total time:\", t1 - t0)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_CLIP_NORM)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "    if not (i % VALIDATE_EVERY):\n",
        "        model.eval()\n",
        "\n",
        "        valid_data = next(iter(val_loader))\n",
        "        valid_loss = 0.\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xl_memories = None\n",
        "            model.knn.clear()\n",
        "            seq, labels = data[:, :-1], data[:, 1:]\n",
        "\n",
        "            for seq_segment, labels_segment in zip(seq.chunk(SEGMENTS, dim = -1), labels.chunk(SEGMENTS, dim = -1)):\n",
        "\n",
        "                loss, xl_memories = model(\n",
        "                    seq_segment,\n",
        "                    labels = labels_segment,\n",
        "                    xl_memories = xl_memories\n",
        "                )\n",
        "\n",
        "                valid_loss += loss.item() / SEGMENTS\n",
        "\n",
        "        print(f'valid loss: {valid_loss}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-17T14:26:16.905268Z",
          "iopub.execute_input": "2024-03-17T14:26:16.906179Z",
          "iopub.status.idle": "2024-03-17T14:46:21.865555Z",
          "shell.execute_reply.started": "2024-03-17T14:26:16.906134Z",
          "shell.execute_reply": "2024-03-17T14:46:21.863619Z"
        },
        "trusted": true,
        "id": "8stckam58rX-",
        "outputId": "cdf8741c-b719-498a-f8cd-374a9c3de327"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "training:   0%|          | 0/200 [00:00<?, ?it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Begin document\ntraining loss: 5.039541959762574\nEnd document, total time: 8.374260902404785\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   0%|          | 1/200 [00:15<50:12, 15.14s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "valid loss: 4.46309003829956\nBegin document\ntraining loss: 4.492447185516358\nEnd document, total time: 7.744511842727661\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   2%|▏         | 3/200 [00:30<31:27,  9.58s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 4.098450136184693\nEnd document, total time: 7.306441783905029\nBegin document\ntraining loss: 3.8796200037002566\nEnd document, total time: 7.242147207260132\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   2%|▎         | 5/200 [00:45<27:28,  8.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.7435310840606686\nEnd document, total time: 7.328748464584351\nBegin document\ntraining loss: 3.616596484184265\nEnd document, total time: 7.472764492034912\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   4%|▎         | 7/200 [01:00<25:56,  8.06s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.5251991748809814\nEnd document, total time: 7.360278844833374\nBegin document\ntraining loss: 3.4646272182464597\nEnd document, total time: 7.3023681640625\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   4%|▍         | 9/200 [01:15<24:51,  7.81s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.438588643074036\nEnd document, total time: 7.276176452636719\nBegin document\ntraining loss: 3.4132254362106322\nEnd document, total time: 7.055574893951416\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   6%|▌         | 11/200 [01:29<24:08,  7.66s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.372542500495911\nEnd document, total time: 7.504054546356201\nBegin document\ntraining loss: 3.334512138366699\nEnd document, total time: 7.23045539855957\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   6%|▋         | 13/200 [01:44<23:27,  7.52s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.29343364238739\nEnd document, total time: 7.05449366569519\nBegin document\ntraining loss: 3.2777815341949466\nEnd document, total time: 7.261979818344116\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   8%|▊         | 15/200 [01:59<23:13,  7.53s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.2410950183868406\nEnd document, total time: 7.6329450607299805\nBegin document\ntraining loss: 3.2314491987228395\nEnd document, total time: 7.266247034072876\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:   8%|▊         | 17/200 [02:14<22:49,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.1802069187164306\nEnd document, total time: 7.261354446411133\nBegin document\ntraining loss: 3.1663676261901856\nEnd document, total time: 7.186507701873779\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  10%|▉         | 19/200 [02:29<22:39,  7.51s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.1007745027542115\nEnd document, total time: 7.765432596206665\nBegin document\ntraining loss: 3.057208847999573\nEnd document, total time: 7.328123569488525\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  10%|█         | 21/200 [02:44<22:16,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 3.0353387594223022\nEnd document, total time: 7.185502767562866\nBegin document\ntraining loss: 3.004630327224731\nEnd document, total time: 7.254883289337158\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  12%|█▏        | 23/200 [02:58<21:55,  7.43s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.996585941314697\nEnd document, total time: 7.224204778671265\nBegin document\ntraining loss: 3.0295111656188958\nEnd document, total time: 7.321697950363159\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  12%|█▎        | 25/200 [03:13<21:34,  7.40s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.937446904182434\nEnd document, total time: 7.1251914501190186\nBegin document\ntraining loss: 2.922499585151672\nEnd document, total time: 7.234288930892944\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  14%|█▎        | 27/200 [03:28<21:17,  7.39s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.856919074058533\nEnd document, total time: 7.2671058177948\nBegin document\ntraining loss: 2.8808781623840334\nEnd document, total time: 7.482014179229736\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  14%|█▍        | 29/200 [03:42<21:01,  7.38s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.8619038343429564\nEnd document, total time: 7.038935661315918\nBegin document\ntraining loss: 2.8057425498962405\nEnd document, total time: 7.161719083786011\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  16%|█▌        | 31/200 [03:57<20:44,  7.36s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.811570572853088\nEnd document, total time: 7.277841329574585\nBegin document\ntraining loss: 2.8176154375076297\nEnd document, total time: 7.4859619140625\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  16%|█▋        | 33/200 [04:12<20:28,  7.36s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.809566187858581\nEnd document, total time: 6.982219934463501\nBegin document\ntraining loss: 2.8008985757827753\nEnd document, total time: 6.822697877883911\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  18%|█▊        | 35/200 [04:26<19:57,  7.26s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.7528455257415767\nEnd document, total time: 7.0360987186431885\nBegin document\ntraining loss: 2.746201181411744\nEnd document, total time: 6.935306549072266\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  18%|█▊        | 37/200 [04:40<19:36,  7.22s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.752862143516541\nEnd document, total time: 7.083675861358643\nBegin document\ntraining loss: 2.7754136562347416\nEnd document, total time: 6.928543567657471\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  20%|█▉        | 39/200 [04:54<19:12,  7.16s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.7197938442230223\nEnd document, total time: 6.909160614013672\nBegin document\ntraining loss: 2.7179848670959474\nEnd document, total time: 6.81702446937561\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  20%|██        | 41/200 [05:08<18:59,  7.17s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.6873924970626826\nEnd document, total time: 7.340386867523193\nBegin document\ntraining loss: 2.6663897991180416\nEnd document, total time: 6.960350751876831\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  22%|██▏       | 43/200 [05:22<18:37,  7.12s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.657948446273804\nEnd document, total time: 6.856670379638672\nBegin document\ntraining loss: 2.6397240877151487\nEnd document, total time: 7.130556583404541\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  22%|██▎       | 45/200 [05:37<18:39,  7.22s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.671142244338989\nEnd document, total time: 7.57828164100647\nBegin document\ntraining loss: 2.6713225126266478\nEnd document, total time: 7.301489591598511\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  24%|██▎       | 47/200 [05:52<18:30,  7.26s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.6617756366729735\nEnd document, total time: 7.182066440582275\nBegin document\ntraining loss: 2.6341784715652463\nEnd document, total time: 7.255206108093262\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  24%|██▍       | 49/200 [06:07<18:20,  7.29s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.6546007156372067\nEnd document, total time: 7.244778394699097\nBegin document\ntraining loss: 2.6411700248718266\nEnd document, total time: 7.437213897705078\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  26%|██▌       | 51/200 [06:21<18:09,  7.31s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.6175357818603517\nEnd document, total time: 7.06542444229126\nBegin document\ntraining loss: 2.6057873249053958\nEnd document, total time: 7.180307865142822\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  26%|██▋       | 53/200 [06:36<17:55,  7.32s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.609294557571411\nEnd document, total time: 7.271184921264648\nBegin document\ntraining loss: 2.5596313238143917\nEnd document, total time: 7.516435623168945\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  28%|██▊       | 55/200 [06:51<17:48,  7.37s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.615690732002258\nEnd document, total time: 7.233374118804932\nBegin document\ntraining loss: 2.6076946258544917\nEnd document, total time: 7.2973339557647705\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  28%|██▊       | 57/200 [07:06<17:34,  7.37s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5570107936859134\nEnd document, total time: 7.268221616744995\nBegin document\ntraining loss: 2.5825605630874633\nEnd document, total time: 7.8143088817596436\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  30%|██▉       | 59/200 [07:21<17:32,  7.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5541709184646604\nEnd document, total time: 7.330108165740967\nBegin document\ntraining loss: 2.6037556886672975\nEnd document, total time: 7.293806314468384\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  30%|███       | 61/200 [07:36<17:16,  7.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5487401485443115\nEnd document, total time: 7.378065824508667\nBegin document\ntraining loss: 2.5489181756973265\nEnd document, total time: 7.425987720489502\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  32%|███▏      | 63/200 [07:51<17:04,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.557217407226563\nEnd document, total time: 7.42017388343811\nBegin document\ntraining loss: 2.556982421875\nEnd document, total time: 7.328909158706665\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  32%|███▎      | 65/200 [08:06<16:48,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5416264533996578\nEnd document, total time: 7.332236051559448\nBegin document\ntraining loss: 2.540660238265991\nEnd document, total time: 7.227803707122803\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  34%|███▎      | 67/200 [08:21<16:34,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5474711894989013\nEnd document, total time: 7.559648752212524\nBegin document\ntraining loss: 2.528946352005005\nEnd document, total time: 7.324854135513306\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  34%|███▍      | 69/200 [08:36<16:17,  7.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.523319864273071\nEnd document, total time: 7.30050802230835\nBegin document\ntraining loss: 2.525755095481872\nEnd document, total time: 7.1444621086120605\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  36%|███▌      | 71/200 [08:51<16:04,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5115158081054685\nEnd document, total time: 7.658086538314819\nBegin document\ntraining loss: 2.522643899917602\nEnd document, total time: 7.3765246868133545\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  36%|███▋      | 73/200 [09:06<15:48,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4932637453079223\nEnd document, total time: 7.326087474822998\nBegin document\ntraining loss: 2.4944551706314084\nEnd document, total time: 7.326231002807617\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  38%|███▊      | 75/200 [09:21<15:39,  7.51s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.461370325088501\nEnd document, total time: 7.698078155517578\nBegin document\ntraining loss: 2.497510313987732\nEnd document, total time: 7.14426064491272\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  38%|███▊      | 77/200 [09:36<15:17,  7.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5114260196685794\nEnd document, total time: 7.283469915390015\nBegin document\ntraining loss: 2.4832695007324217\nEnd document, total time: 7.253715753555298\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  40%|███▉      | 79/200 [09:50<14:56,  7.41s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.479548573493957\nEnd document, total time: 7.111997127532959\nBegin document\ntraining loss: 2.4798885107040407\nEnd document, total time: 7.357863903045654\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  40%|████      | 81/200 [10:05<14:40,  7.40s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5012339115142823\nEnd document, total time: 7.181897163391113\nBegin document\ntraining loss: 2.521123480796814\nEnd document, total time: 7.233805894851685\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  42%|████▏     | 83/200 [10:20<14:21,  7.36s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.5215580701827998\nEnd document, total time: 7.115772724151611\nBegin document\ntraining loss: 2.4796427726745605\nEnd document, total time: 7.589867115020752\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  42%|████▎     | 85/200 [10:35<14:09,  7.39s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4966239213943484\nEnd document, total time: 7.100458383560181\nBegin document\ntraining loss: 2.4449331283569333\nEnd document, total time: 7.224069595336914\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  44%|████▎     | 87/200 [10:49<13:53,  7.38s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4806287765502932\nEnd document, total time: 7.273103713989258\nBegin document\ntraining loss: 2.4743970394134522\nEnd document, total time: 7.631104946136475\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  44%|████▍     | 89/200 [11:04<13:41,  7.40s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4577834367752076\nEnd document, total time: 7.062979221343994\nBegin document\ntraining loss: 2.527753615379333\nEnd document, total time: 7.20804762840271\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  46%|████▌     | 91/200 [11:19<13:23,  7.38s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.494409966468811\nEnd document, total time: 7.207025766372681\nBegin document\ntraining loss: 2.4606940031051634\nEnd document, total time: 7.560499906539917\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  46%|████▋     | 93/200 [11:34<13:10,  7.38s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4756375312805177\nEnd document, total time: 7.029901742935181\nBegin document\ntraining loss: 2.441540575027466\nEnd document, total time: 6.931883335113525\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  48%|████▊     | 95/200 [11:48<12:45,  7.29s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4428407669067385\nEnd document, total time: 6.988785028457642\nBegin document\ntraining loss: 2.542115473747253\nEnd document, total time: 6.953532695770264\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  48%|████▊     | 97/200 [12:02<12:29,  7.28s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.442001295089722\nEnd document, total time: 7.362692356109619\nBegin document\ntraining loss: 2.4514965534210207\nEnd document, total time: 6.967535018920898\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  50%|████▉     | 99/200 [12:16<12:09,  7.23s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4846585750579835\nEnd document, total time: 7.029708385467529\nBegin document\ntraining loss: 2.494274663925171\nEnd document, total time: 7.038853406906128\nBegin document\ntraining loss: 2.468542075157165\nEnd document, total time: 7.2282140254974365\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  50%|█████     | 101/200 [12:37<13:25,  8.13s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "valid loss: 2.4635545253753666\nBegin document\ntraining loss: 2.5085588693618774\nEnd document, total time: 6.922603130340576\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  52%|█████▏    | 103/200 [12:51<12:39,  7.83s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.44498507976532\nEnd document, total time: 7.098491191864014\nBegin document\ntraining loss: 2.4806080579757688\nEnd document, total time: 6.957918643951416\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  52%|█████▎    | 105/200 [13:06<12:08,  7.67s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4577840089797975\nEnd document, total time: 7.422662973403931\nBegin document\ntraining loss: 2.475238013267517\nEnd document, total time: 7.215893507003784\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  54%|█████▎    | 107/200 [13:20<11:43,  7.57s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4810818910598753\nEnd document, total time: 7.245092391967773\nBegin document\ntraining loss: 2.4417475938797\nEnd document, total time: 7.263494491577148\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  55%|█████▍    | 109/200 [13:36<11:29,  7.58s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4658073663711546\nEnd document, total time: 7.739418029785156\nBegin document\ntraining loss: 2.4672456741333013\nEnd document, total time: 7.017959117889404\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  56%|█████▌    | 111/200 [13:50<11:06,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.452367854118347\nEnd document, total time: 7.279255628585815\nBegin document\ntraining loss: 2.5203794717788695\nEnd document, total time: 7.164864540100098\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  56%|█████▋    | 113/200 [14:05<10:51,  7.49s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.438669443130493\nEnd document, total time: 7.605443239212036\nBegin document\ntraining loss: 2.4727033138275147\nEnd document, total time: 7.244235992431641\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  57%|█████▊    | 115/200 [14:20<10:31,  7.43s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4092563390731807\nEnd document, total time: 7.167381048202515\nBegin document\ntraining loss: 2.4299096822738653\nEnd document, total time: 7.200842380523682\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  58%|█████▊    | 117/200 [14:34<10:13,  7.39s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4222781658172607\nEnd document, total time: 7.185885906219482\nBegin document\ntraining loss: 2.43513011932373\nEnd document, total time: 7.556690454483032\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  60%|█████▉    | 119/200 [14:50<10:03,  7.45s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4420917749404905\nEnd document, total time: 7.366397142410278\nBegin document\ntraining loss: 2.435632276535034\nEnd document, total time: 7.236774921417236\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  60%|██████    | 121/200 [15:04<09:46,  7.42s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4051583528518674\nEnd document, total time: 7.2835612297058105\nBegin document\ntraining loss: 2.4559593915939333\nEnd document, total time: 7.774540185928345\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  62%|██████▏   | 123/200 [15:20<09:37,  7.50s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.446843147277832\nEnd document, total time: 7.357535123825073\nBegin document\ntraining loss: 2.442533397674561\nEnd document, total time: 7.3488781452178955\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  62%|██████▎   | 125/200 [15:34<09:20,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.418792414665222\nEnd document, total time: 7.249176740646362\nBegin document\ntraining loss: 2.4549681901931764\nEnd document, total time: 7.819297790527344\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  64%|██████▎   | 127/200 [15:50<09:09,  7.53s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4535364389419554\nEnd document, total time: 7.283367395401001\nBegin document\ntraining loss: 2.4181143045425415\nEnd document, total time: 7.31215763092041\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  64%|██████▍   | 129/200 [16:05<08:51,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4734151363372803\nEnd document, total time: 7.241044998168945\nBegin document\ntraining loss: 2.423294734954834\nEnd document, total time: 7.424448490142822\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  66%|██████▌   | 131/200 [16:19<08:34,  7.46s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4382560253143315\nEnd document, total time: 7.153767347335815\nBegin document\ntraining loss: 2.437877726554871\nEnd document, total time: 7.1403725147247314\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  66%|██████▋   | 133/200 [16:34<08:16,  7.41s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4689619064331056\nEnd document, total time: 7.217736005783081\nBegin document\ntraining loss: 2.390569567680359\nEnd document, total time: 7.051027059555054\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  68%|██████▊   | 135/200 [16:49<08:01,  7.41s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.369512414932251\nEnd document, total time: 7.586649656295776\nBegin document\ntraining loss: 2.4353806972503667\nEnd document, total time: 7.249579429626465\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  68%|██████▊   | 137/200 [17:03<07:46,  7.40s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4548264503479005\nEnd document, total time: 7.252155065536499\nBegin document\ntraining loss: 2.4464282751083375\nEnd document, total time: 7.142266035079956\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  70%|██████▉   | 139/200 [17:19<07:35,  7.47s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4422513961791994\nEnd document, total time: 7.8954386711120605\nBegin document\ntraining loss: 2.500449371337891\nEnd document, total time: 7.143323659896851\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  70%|███████   | 141/200 [17:33<07:17,  7.42s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4341185569763186\nEnd document, total time: 7.2466278076171875\nBegin document\ntraining loss: 2.4435135602951052\nEnd document, total time: 7.287298917770386\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  72%|███████▏  | 143/200 [17:49<07:06,  7.48s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4254390239715575\nEnd document, total time: 7.759059190750122\nBegin document\ntraining loss: 2.4071861743927006\nEnd document, total time: 7.2668962478637695\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  72%|███████▎  | 145/200 [18:03<06:49,  7.45s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4406036138534546\nEnd document, total time: 7.2666168212890625\nBegin document\ntraining loss: 2.406069707870483\nEnd document, total time: 7.278563737869263\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  74%|███████▎  | 147/200 [18:18<06:33,  7.43s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.437677597999573\nEnd document, total time: 7.282994747161865\nBegin document\ntraining loss: 2.4406843423843383\nEnd document, total time: 7.507497310638428\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  74%|███████▍  | 149/200 [18:33<06:18,  7.43s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.414120817184448\nEnd document, total time: 7.144364595413208\nBegin document\ntraining loss: 2.4483897447586056\nEnd document, total time: 7.252863883972168\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  76%|███████▌  | 151/200 [18:48<06:03,  7.41s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4357647657394406\nEnd document, total time: 7.266793966293335\nBegin document\ntraining loss: 2.4323696851730343\nEnd document, total time: 7.618861675262451\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  76%|███████▋  | 153/200 [19:03<05:48,  7.42s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.434057831764221\nEnd document, total time: 7.063844203948975\nBegin document\ntraining loss: 2.400449752807617\nEnd document, total time: 7.07490086555481\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  78%|███████▊  | 155/200 [19:17<05:30,  7.35s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.482611656188965\nEnd document, total time: 7.075734376907349\nBegin document\ntraining loss: 2.4189081430435184\nEnd document, total time: 7.592996597290039\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  78%|███████▊  | 157/200 [19:32<05:15,  7.34s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.433962678909302\nEnd document, total time: 6.840792655944824\nBegin document\ntraining loss: 2.405925035476684\nEnd document, total time: 7.0137104988098145\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  80%|███████▉  | 159/200 [19:46<04:57,  7.25s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.4208086252212526\nEnd document, total time: 6.870612382888794\nBegin document\ntraining loss: 2.4255853891372676\nEnd document, total time: 6.945481061935425\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  80%|████████  | 161/200 [20:00<04:41,  7.23s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "training loss: 2.397289824485779\nEnd document, total time: 7.173508405685425\nBegin document\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "training:  80%|████████  | 161/200 [20:04<04:51,  7.48s/it]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Each pass will be (BATCH_SIZE * SEGMENTS) iterations\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_segment, labels_segment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seq\u001b[38;5;241m.\u001b[39mchunk(SEGMENTS, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mchunk(SEGMENTS, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 28\u001b[0m     loss, xl_memories \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_segment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels_segment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxl_memories\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxl_memories\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m SEGMENTS\n\u001b[1;32m     35\u001b[0m     (loss \u001b[38;5;241m/\u001b[39m SEGMENTS)\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[18], line 138\u001b[0m, in \u001b[0;36mMemorizingTransformer.forward\u001b[0;34m(self, x, relative_positions, xl_memories, labels)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     layer_rel_pos \u001b[38;5;241m=\u001b[39m rel_pos\n\u001b[0;32m--> 138\u001b[0m x, xl_mem \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mxl_memories_iter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_rel_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xl_mem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     new_xl_memories\u001b[38;5;241m.\u001b[39mappend(xl_mem\u001b[38;5;241m.\u001b[39mdetach())\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[17], line 56\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, xl_memories, rel_pos)\u001b[0m\n\u001b[1;32m     54\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     55\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m---> 56\u001b[0m attn_out, new_xl_memories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxl_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxl_memories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m attn_out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[1;32m     59\u001b[0m residual \u001b[38;5;241m=\u001b[39m attn_out\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[16], line 118\u001b[0m, in \u001b[0;36mKNNAttention.forward\u001b[0;34m(self, x, relative_positions, xl_memory)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mntotal \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    117\u001b[0m     queries \u001b[38;5;241m=\u001b[39m rearrange(queries, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h t d -> b t (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m     mem_kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk_retrieved_memories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     mem_k, mem_v \u001b[38;5;241m=\u001b[39m mem_kv\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    120\u001b[0m     mem_k \u001b[38;5;241m=\u001b[39m rearrange(mem_k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb t k (h d) -> b h t k d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads)\n",
            "Cell \u001b[0;32mIn[6], line 110\u001b[0m, in \u001b[0;36mKNN.search\u001b[0;34m(self, query_vecs, topk)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Input is b n d, flatten to (b n) d\u001b[39;00m\n\u001b[1;32m    109\u001b[0m query_vecs \u001b[38;5;241m=\u001b[39m query_vecs\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m kvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_and_retrieve(np\u001b[38;5;241m.\u001b[39mascontiguousarray(\u001b[43mquery_vecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()), topk)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# kvs are (b n) k 2 d, unflatten to b n k 2 d\u001b[39;00m\n\u001b[1;32m    112\u001b[0m kvs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(kvs)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAcbOu4a8rX_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}